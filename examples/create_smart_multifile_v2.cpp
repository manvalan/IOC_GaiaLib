/**
 * Smart Multi-File V2 Catalog Creator
 * Creates directory structure with individual chunk files
 * Decompresses chunks on-demand for instant access
 */

#include <iostream>
#include <fstream>
#include <filesystem>
#include <vector>
#include <string>
#include <chrono>
#include <zlib.h>
#include <thread>
#include <future>
#include <cstring>
#include "ioc_gaialib/gaia_mag18_catalog_v2.h"

using namespace std;
using namespace ioc::gaia;
namespace fs = std::filesystem;

struct ChunkDecompressionJob {
    uint64_t chunk_id;
    uint64_t file_offset;
    uint32_t compressed_size;
    uint32_t uncompressed_size;
    uint32_t num_stars;
    string output_path;
};

bool decompressChunk(const string& catalog_file, const ChunkDecompressionJob& job) {
    // Open catalog file
    FILE* file = fopen(catalog_file.c_str(), "rb");
    if (!file) {
        cerr << "Failed to open catalog file\n";
        return false;
    }
    
    // Seek to chunk data
    if (fseek(file, job.file_offset, SEEK_SET) != 0) {
        cerr << "Failed to seek to chunk " << job.chunk_id << "\n";
        fclose(file);
        return false;
    }
    
    // Read compressed data
    vector<uint8_t> compressed(job.compressed_size);
    if (fread(compressed.data(), 1, job.compressed_size, file) != job.compressed_size) {
        cerr << "Failed to read chunk " << job.chunk_id << "\n";
        fclose(file);
        return false;
    }
    fclose(file);
    
    // Decompress
    vector<uint8_t> uncompressed(job.uncompressed_size);
    uLongf dest_len = job.uncompressed_size;
    int result = uncompress(uncompressed.data(), &dest_len, compressed.data(), job.compressed_size);
    
    if (result != Z_OK) {
        cerr << "Decompression failed for chunk " << job.chunk_id << " (zlib error " << result << ")\n";
        return false;
    }
    
    // Write uncompressed data
    ofstream out(job.output_path, ios::binary);
    if (!out) {
        cerr << "Failed to create output file: " << job.output_path << "\n";
        return false;
    }
    
    out.write(reinterpret_cast<const char*>(uncompressed.data()), dest_len);
    out.close();
    
    return true;
}

int main(int argc, char* argv[]) {
    if (argc != 3) {
        cout << "Usage: " << argv[0] << " <input.mag18v2> <output_directory>\n\n";
        cout << "Creates multi-file structure for instant V2 access:\n\n";
        cout << "  <output_directory>/\n";
        cout << "  â”œâ”€â”€ metadata.dat          (header + indices, ~1MB)\n";  
        cout << "  â””â”€â”€ chunks/\n";
        cout << "      â”œâ”€â”€ chunk_000.dat    (~80MB uncompressed)\n";
        cout << "      â”œâ”€â”€ chunk_001.dat    (~80MB uncompressed)\n";
        cout << "      â””â”€â”€ ...\n\n";
        cout << "Benefits:\n";
        cout << "  âœ… Instant cone searches (<10ms)\n";
        cout << "  âœ… Parallel chunk loading\n";
        cout << "  âœ… Cache-friendly (OS file cache)\n";
        cout << "  âœ… Scalable (only load needed chunks)\n\n";
        cout << "Total size: ~18GB (vs 9GB compressed)\n";
        return 1;
    }
    
    string input_catalog = argv[1];
    string output_dir = argv[2];
    
    cout << "\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n";
    cout << "â•‘  Smart Multi-File V2 Catalog Creator                      â•‘\n";
    cout << "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n";
    
    auto overall_start = chrono::high_resolution_clock::now();
    
    try {
        // Step 1: Load and analyze catalog
        cout << "ðŸ“– Step 1: Analyzing compressed catalog\n";
        cout << "   Input: " << input_catalog << "\n";
        
        // We need to extract header and indices manually since Mag18CatalogV2 doesn't expose chunk info
        FILE* input_file = fopen(input_catalog.c_str(), "rb");
        if (!input_file) {
            cerr << "âŒ Failed to open input catalog\n";
            return 1;
        }
        
        // Read header
        Mag18CatalogHeaderV2 header;
        if (fread(&header, sizeof(header), 1, input_file) != 1) {
            cerr << "âŒ Failed to read header\n";
            fclose(input_file);
            return 1;
        }
        
        cout << "   âœ… Catalog: " << header.total_stars << " stars, " << header.total_chunks << " chunks\n";
        cout << "   âœ… HEALPix: NSIDE=" << header.healpix_nside << ", " << header.num_healpix_pixels << " pixels\n\n";
        
        // Read HEALPix index
        fseek(input_file, header.healpix_index_offset, SEEK_SET);
        vector<HEALPixIndexEntry> healpix_index(header.num_healpix_pixels);
        fread(healpix_index.data(), sizeof(HEALPixIndexEntry), header.num_healpix_pixels, input_file);
        
        // Read chunk index
        fseek(input_file, header.chunk_index_offset, SEEK_SET);
        vector<ChunkInfo> chunk_index(header.total_chunks);
        fread(chunk_index.data(), sizeof(ChunkInfo), header.total_chunks, input_file);
        
        fclose(input_file);
        
        // Step 2: Create directory structure
        cout << "ðŸ“ Step 2: Creating directory structure\n";
        cout << "   Output: " << output_dir << "\n";
        
        fs::create_directories(output_dir);
        fs::create_directories(output_dir + "/chunks");
        cout << "   âœ… Directories created\n\n";
        
        // Step 3: Write metadata file
        cout << "ðŸ“ Step 3: Writing metadata\n";
        string metadata_path = output_dir + "/metadata.dat";
        ofstream metadata(metadata_path, ios::binary);
        
        metadata.write(reinterpret_cast<const char*>(&header), sizeof(header));
        metadata.write(reinterpret_cast<const char*>(healpix_index.data()), 
                      header.num_healpix_pixels * sizeof(HEALPixIndexEntry));
        metadata.write(reinterpret_cast<const char*>(chunk_index.data()),
                      header.total_chunks * sizeof(ChunkInfo));
        metadata.close();
        
        cout << "   âœ… Metadata written (" << fs::file_size(metadata_path) / 1024 << " KB)\n\n";
        
        // Step 4: Prepare chunk jobs
        cout << "ðŸ§© Step 4: Preparing chunk decompression jobs\n";
        vector<ChunkDecompressionJob> jobs;
        uint64_t total_uncompressed_size = 0;
        
        for (uint64_t i = 0; i < header.total_chunks; ++i) {
            ChunkDecompressionJob job;
            job.chunk_id = i;
            job.file_offset = chunk_index[i].file_offset;
            job.compressed_size = chunk_index[i].compressed_size;
            job.uncompressed_size = chunk_index[i].uncompressed_size;
            job.num_stars = chunk_index[i].num_stars;
            job.output_path = output_dir + "/chunks/chunk_" + 
                             string(3 - to_string(i).length(), '0') + to_string(i) + ".dat";
            
            jobs.push_back(job);
            total_uncompressed_size += job.uncompressed_size;
        }
        
        cout << "   âœ… " << jobs.size() << " chunks to decompress\n";
        cout << "   âœ… Total uncompressed size: " << (total_uncompressed_size / (1024*1024*1024)) << " GB\n\n";
        
        // Step 5: Decompress chunks in parallel
        cout << "âš¡ Step 5: Decompressing chunks (parallel)\n";
        cout << "   Using " << thread::hardware_concurrency() << " threads\n\n";
        
        const size_t num_threads = thread::hardware_concurrency();
        vector<future<bool>> futures;
        size_t completed = 0;
        auto decompression_start = chrono::high_resolution_clock::now();
        
        // Process chunks in batches to avoid too many threads
        for (size_t batch_start = 0; batch_start < jobs.size(); batch_start += num_threads) {
            size_t batch_end = min(batch_start + num_threads, jobs.size());
            futures.clear();
            
            // Launch threads for this batch
            for (size_t i = batch_start; i < batch_end; ++i) {
                futures.push_back(async(launch::async, decompressChunk, input_catalog, jobs[i]));
            }
            
            // Wait for batch completion and show progress
            for (size_t i = 0; i < futures.size(); ++i) {
                bool success = futures[i].get();
                completed++;
                
                if (success) {
                    cout << "  âœ… Chunk " << (batch_start + i) << "/" << jobs.size() 
                         << " (" << (completed * 100 / jobs.size()) << "%) - " 
                         << fs::file_size(jobs[batch_start + i].output_path) / (1024*1024) << " MB\n";
                } else {
                    cout << "  âŒ Chunk " << (batch_start + i) << " FAILED\n";
                }
            }
        }
        
        auto decompression_end = chrono::high_resolution_clock::now();
        auto decompression_ms = chrono::duration_cast<chrono::milliseconds>(decompression_end - decompression_start).count();
        
        cout << "\n  âœ… All chunks decompressed in " << (decompression_ms / 1000.0) << " seconds\n\n";
        
        // Step 6: Validation and summary
        cout << "ðŸ” Step 6: Validation\n";
        
        // Count output files
        size_t chunk_files = 0;
        uint64_t total_output_size = 0;
        
        for (const auto& entry : fs::directory_iterator(output_dir + "/chunks")) {
            if (entry.path().extension() == ".dat") {
                chunk_files++;
                total_output_size += fs::file_size(entry.path());
            }
        }
        
        cout << "   âœ… Created " << chunk_files << "/" << header.total_chunks << " chunk files\n";
        cout << "   âœ… Total size: " << (total_output_size / (1024*1024*1024)) << " GB\n";
        cout << "   âœ… Metadata: " << (fs::file_size(metadata_path) / 1024) << " KB\n\n";
        
        auto overall_end = chrono::high_resolution_clock::now();
        auto overall_ms = chrono::duration_cast<chrono::milliseconds>(overall_end - overall_start).count();
        
        cout << "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n";
        cout << "â•‘ Multi-File V2 Catalog Created Successfully!               â•‘\n";
        cout << "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n";
        cout << "ðŸ“ Output directory: " << output_dir << "\n";
        cout << "â±ï¸  Total time: " << (overall_ms / 1000.0) << " seconds\n";
        cout << "ðŸš€ Speed: " << (total_output_size / (1024*1024) / (overall_ms / 1000.0)) << " MB/s\n\n";
        
        cout << "Next step: Create reader class that loads chunks on-demand!\n";
        cout << "Expected performance: <10ms cone searches ðŸš€\n";
        
        return 0;
        
    } catch (const exception& e) {
        cerr << "âŒ Error: " << e.what() << "\n";
        return 1;
    }
}